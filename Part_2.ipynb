{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime, timedelta\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "If you made it up to here all by yourself, you can use your prepared dataset to train an Algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010-05-25'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime, timedelta\n",
    "# +3 leap years, -1 - tomorrow \n",
    "str(datetime.datetime.today()- datetime.timedelta(days=11*365 + 3 - 1)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are allowed to use any library you are comfortable with such as sklearn, tensorflow keras etc. \n",
    "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes from .csv-files (saved in Part 1)\n",
    "weather_data_cleaned_all = pd.read_csv('weather_data_cleaned_all.csv', sep=',', index_col=0).sort_index().reset_index(drop=True)\n",
    "train = pd.read_csv('train.csv', sep=',', index_col=0).sort_index().reset_index(drop=True)\n",
    "_eval = pd.read_csv('_eval.csv', sep=',', index_col=0).sort_index().reset_index(drop=True)\n",
    "test = pd.read_csv('test.csv', sep=',', index_col=0).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_cleaned_all['date'] = pd.to_datetime(weather_data_cleaned_all.date)\n",
    "train['date'] = pd.to_datetime(train.date)\n",
    "_eval['date'] = pd.to_datetime(_eval.date)\n",
    "test['date'] = pd.to_datetime(test.date)\n",
    "\n",
    "test_new = test.append(_eval, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wanted date: 2010-05-25.\n"
     ]
    }
   ],
   "source": [
    "# +3 leap years, -1 - tomorrow \n",
    "wanted_date = str(datetime.datetime.today()- datetime.timedelta(days=11*365 + 3 - 1)).split(' ')[0]\n",
    "print(\"Wanted date: \" + wanted_date + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available dates are between 2005-01-01 and 2010-04-15.\n",
      "So, the latest date is 2010-04-15 and it is earlier as our wanted date 2010-05-25.\n"
     ]
    }
   ],
   "source": [
    "all_dates = weather_data_cleaned_all[\"date\"].dt.date\n",
    "\n",
    "min_date = str(all_dates.min())\n",
    "max_date = str(all_dates.max())\n",
    "print(\"The available dates are between \" + min_date + \" and \" + max_date + \".\")\n",
    "print(\"So, the latest date is \" + max_date + \" and it is earlier as our wanted date \" + wanted_date + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of what we don't have any data for the day before the desired forecast day, we could try to generate this data. Unfortunately, only 6 years of data are available, which is not enough to generalize. But we can try, for example, to calculate the mean or median of values from known years for float values, and for boolean use the values that occur most frequently in earlier years.\n",
    "But in the algorithms below I used the data from training and test sets that I saved in part 1.\n",
    "\n",
    "Another possibility would be to use only 'station_number' and 'date' as features, but then we ignore all weather parameters, which cannot be a good solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I change the datatype of 'station_number' from int to category, \n",
    "because these data are discrete, not continuous.\n",
    "\n",
    "Another possibility would be to divide the train and test sets into subsets, \n",
    "individually for each station; \n",
    "and then fit each model with data from each station separately. \n",
    "But in this case we will have too small data and cannot generalize weather-data combinations \n",
    "between the stations. \n",
    "'''\n",
    "weather_data_cleaned_all['station_number'] = weather_data_cleaned_all['station_number'].astype('category')\n",
    "train['station_number'] = train['station_number'].astype('category')\n",
    "_eval['station_number'] = _eval['station_number'].astype('category')\n",
    "test['station_number'] = test['station_number'].astype('category')\n",
    "test_new['station_number'] = test_new['station_number'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_np(train, test):\n",
    "    \n",
    "    train['date'] = train['date'].dt.dayofyear\n",
    "    test['date'] = test['date'].dt.dayofyear\n",
    "    \n",
    "    feature_list = ['station_number',\n",
    "                    'mean_temp',\n",
    "                    'mean_dew_point',\n",
    "                    'mean_sealevel_pressure',\n",
    "                    'mean_visibility',\n",
    "                    'mean_wind_speed',\n",
    "                    'max_sustained_wind_speed',\n",
    "                    'max_temperature',\n",
    "                    'total_precipitation',\n",
    "                    'fog',\n",
    "                    'rain',\n",
    "                    'snow',\n",
    "                    'hail',\n",
    "                    'thunder',\n",
    "                    'tornado',\n",
    "                    'date',\n",
    "                    'snow_tomorrow']\n",
    "    \n",
    "    feature_list_ind = len(feature_list) - 1\n",
    "    \n",
    "    train_np = train.to_numpy()\n",
    "    \n",
    "    X_train = train_np[:, :feature_list_ind]\n",
    "    y_train = train_np[:, feature_list_ind]\n",
    "    \n",
    "    test_np = test.to_numpy()\n",
    "    \n",
    "    X_test = test_np[:, :feature_list_ind]\n",
    "    y_test = test_np[:, feature_list_ind]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=3000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Logistic regression score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"SVM score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Random Forest score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_train_test_np(train, test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With original data:\n",
      "\n",
      "Logistic regression score: 0.8738738738738738\n",
      "SVM score: 0.8706442291347952\n",
      "Random Forest score: 0.8791432942376338\n"
     ]
    }
   ],
   "source": [
    "print(\"With original data:\\n\")\n",
    "logistic_regression(X_train, y_train, X_test, y_test)\n",
    "svm_(X_train, y_train, X_test, y_test)\n",
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these algorithms have accuracy score around 87%. But are they really so good?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14631\n",
       "1     2175\n",
       "Name: snow_tomorrow, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data_cleaned_all['snow_tomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is the imbalance: we have much more days without snowfall as with snowfall. This means that even if these algorithms are always predicting \"no snow\", these predictions will be mostly correct.\n",
    "\n",
    "To eliminate this imbalance, we can either delete random data with 'snow_tomorrow' == False, or copy random data with 'snow_tomorrow' == True several times, so that in the end there is the same amount of data with and without snow. Deleting will greatly reduce the number of data, so I'm trying to use the copy option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_data(X_train, y_train, X_test, y_test):    \n",
    "   \n",
    "    y_train_1_idx = np.where(y_train == 1)[0] # Indices of data with 'snow_tomorrow' == True\n",
    "    m_train = random.choices(y_train_1_idx, k=len(y_train) - len(y_train_1_idx))   \n",
    "    \n",
    "    X_train = np.append(X_train, X_train[m_train], axis=0)\n",
    "    y_train = np.append(y_train, y_train[m_train], axis=0)\n",
    "    \n",
    "    y_test_1_idx = np.where(y_test == 1)[0] # Indices of data with 'snow_tomorrow' == True\n",
    "    m_test = random.choices(y_test_1_idx, k=len(y_test) - len(y_test_1_idx))\n",
    "    \n",
    "    X_test = np.append(X_test, X_test[m_test], axis=0)\n",
    "    y_test = np.append(y_test, y_test[m_test], axis=0)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced = create_balanced_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With balanced data:\n",
      "\n",
      "Logistic regression score: 0.6654248069059518\n",
      "SVM score: 0.5345751930940481\n",
      "Random Forest score: 0.5769195820081781\n"
     ]
    }
   ],
   "source": [
    "print(\"With balanced data:\\n\")\n",
    "logistic_regression(X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced)\n",
    "svm_(X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced)\n",
    "random_forest(X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracies have become significantly worse, but at least they are over 50%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
